#!/usr/bin/env python3
"""
Validation script for Advanced BEM Variants implementation.

Validates that all V2, V7, V11 architectures are properly implemented
and can be instantiated without errors.
"""

import sys
import traceback
from pathlib import Path

# Add project root to path
sys.path.append(str(Path(__file__).parent))

def test_imports():
    """Test that all advanced variant modules can be imported."""
    print("üîç Testing imports...")
    
    try:
        # Test BEM module imports
        from bem import (
            BEMv11Stable,
            AdvancedBEMFactory,
            AdvancedVariantsRunner,
            DualPathLoRA,
            MultiLayerDualPathLoRA,
            FiLMConditioner,
            FiLMLiteBEM,
            CachePolicyController,
            LearnedCacheBEM
        )
        
        # Test individual module imports
        from bem.modules.dual_path_lora import (
            create_dual_path_lora_for_model,
            OrthogonalityRegularizer,
            GateDecorrelationLoss
        )
        
        from bem.modules.film_lite import (
            create_film_lite_bem_for_model,
            FiLMEnhancedBEMLayer
        )
        
        from bem.modules.learned_cache_policy import (
            create_learned_cache_bem_for_model,
            LearnedCacheKVLayer
        )
        
        from bem.advanced_variants import AdvancedBEMFactory
        
        print("‚úÖ All imports successful!")
        return True
        
    except Exception as e:
        print(f"‚ùå Import failed: {e}")
        traceback.print_exc()
        return False

def test_architecture_factory():
    """Test the AdvancedBEMFactory functionality."""
    print("\nüîç Testing AdvancedBEMFactory...")
    
    try:
        # Test supported architectures
        supported = AdvancedBEMFactory.SUPPORTED_ARCHITECTURES
        expected_archs = [
            'bem_v11_stable',
            'dual_path_lora', 
            'film_lite_bem',
            'learned_cache_bem'
        ]
        
        for arch in expected_archs:
            if arch not in supported:
                print(f"‚ùå Missing architecture: {arch}")
                return False
                
        print(f"‚úÖ All {len(expected_archs)} architectures supported")
        print("Supported architectures:")
        for arch, desc in supported.items():
            print(f"  - {arch}: {desc}")
            
        return True
        
    except Exception as e:
        print(f"‚ùå Factory test failed: {e}")
        traceback.print_exc()
        return False

def test_experiment_configs():
    """Test that experiment configurations exist."""
    print("\nüîç Testing experiment configurations...")
    
    required_configs = [
        "experiments/B1_bem_v11_stable.yaml",
        "experiments/V2_dual_path.yaml",
        "experiments/V7_film_lite.yaml",
        "experiments/V11_learned_cache_policy.yaml"
    ]
    
    existing_configs = []
    missing_configs = []
    
    for config_path in required_configs:
        if Path(config_path).exists():
            existing_configs.append(config_path)
            print(f"‚úÖ Found: {config_path}")
        else:
            missing_configs.append(config_path)
            print(f"‚ùå Missing: {config_path}")
            
    if missing_configs:
        print(f"\n‚ö†Ô∏è  {len(missing_configs)} configurations missing")
        print("These are required for running experiments")
        return False, existing_configs
    else:
        print(f"\n‚úÖ All {len(existing_configs)} configurations found")
        return True, existing_configs

def test_campaign_runner():
    """Test the campaign runner can be instantiated."""
    print("\nüîç Testing campaign runner...")
    
    try:
        from run_advanced_variants_campaign import AdvancedVariantsCampaignExecutor
        
        # Try to create executor
        executor = AdvancedVariantsCampaignExecutor(
            experiments_dir="experiments",
            output_dir="logs/test_advanced_variants",
            num_seeds=2,  # Small test
            max_parallel=1
        )
        
        print("‚úÖ Campaign executor created successfully")
        print(f"   - Experiments dir: {executor.experiments_dir}")
        print(f"   - Output dir: {executor.output_dir}")
        print(f"   - Seeds: {executor.num_seeds}")
        print(f"   - Max parallel: {executor.max_parallel}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Campaign runner test failed: {e}")
        traceback.print_exc()
        return False

def test_quality_gates():
    """Test quality gate definitions."""
    print("\nüîç Testing quality gates...")
    
    try:
        from bem.advanced_variants import AdvancedVariantsRunner
        
        runner = AdvancedVariantsRunner()
        gates = runner.quality_gates
        
        expected_variants = ['V2_dual_path', 'V7_film_lite', 'V11_learned_cache']
        
        for variant in expected_variants:
            if variant not in gates:
                print(f"‚ùå Missing quality gates for: {variant}")
                return False
            else:
                gate_count = len(gates[variant])
                print(f"‚úÖ {variant}: {gate_count} quality gates defined")
                
        print("‚úÖ All quality gates properly defined")
        return True
        
    except Exception as e:
        print(f"‚ùå Quality gates test failed: {e}")
        traceback.print_exc()
        return False

def run_dry_run_test():
    """Run a dry-run test of the campaign."""
    print("\nüîç Running dry-run test...")
    
    try:
        # Import the campaign script
        from run_advanced_variants_campaign import AdvancedVariantsCampaignExecutor
        
        # Create executor
        executor = AdvancedVariantsCampaignExecutor(
            experiments_dir="experiments",
            output_dir="logs/test_dry_run",
            num_seeds=1,  # Minimal for dry run
            max_parallel=1
        )
        
        # Verify configurations
        config_files = executor.verify_configurations()
        
        if config_files:
            # Create jobs (dry run)
            jobs = executor.create_experiment_jobs(config_files)
            print(f"‚úÖ Dry run successful: would create {len(jobs)} jobs")
            
            # Show job breakdown
            job_breakdown = {}
            for job in jobs:
                method = job.method_type
                if method not in job_breakdown:
                    job_breakdown[method] = 0
                job_breakdown[method] += 1
                
            print("Job breakdown:")
            for method, count in job_breakdown.items():
                print(f"  - {method}: {count} jobs")
                
            return True
        else:
            print("‚ùå Dry run failed: no valid configurations found")
            return False
            
    except Exception as e:
        print(f"‚ùå Dry run test failed: {e}")
        traceback.print_exc()
        return False

def main():
    """Run all validation tests."""
    print("="*60)
    print("ADVANCED BEM VARIANTS VALIDATION")
    print("="*60)
    
    tests = [
        ("Import Test", test_imports),
        ("Architecture Factory Test", test_architecture_factory), 
        ("Experiment Configs Test", test_experiment_configs),
        ("Campaign Runner Test", test_campaign_runner),
        ("Quality Gates Test", test_quality_gates),
        ("Dry Run Test", run_dry_run_test)
    ]
    
    results = []
    
    for test_name, test_func in tests:
        try:
            if test_name == "Experiment Configs Test":
                success, configs = test_func()
                results.append((test_name, success))
                if not success:
                    print(f"\n‚ö†Ô∏è  {test_name} found missing configs, but continuing validation...")
            else:
                success = test_func()
                results.append((test_name, success))
        except Exception as e:
            print(f"\n‚ùå {test_name} crashed: {e}")
            results.append((test_name, False))
    
    # Print summary
    print("\n" + "="*60)
    print("VALIDATION SUMMARY")
    print("="*60)
    
    passed = 0
    total = len(results)
    
    for test_name, success in results:
        status = "‚úÖ PASS" if success else "‚ùå FAIL"
        print(f"{status} {test_name}")
        if success:
            passed += 1
    
    print(f"\nOverall: {passed}/{total} tests passed ({passed/total*100:.1f}%)")
    
    if passed == total:
        print("\nüéâ All validation tests passed! Advanced variants are ready.")
        return 0
    elif passed >= total - 1:  # Allow for missing configs
        print("\n‚ö†Ô∏è  Minor issues found but implementation appears functional.")
        print("   Advanced variants should work with proper experiment configs.")
        return 0
    else:
        print("\n‚ùå Significant validation failures. Please fix before proceeding.")
        return 1

if __name__ == "__main__":
    sys.exit(main())