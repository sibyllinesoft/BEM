#!/usr/bin/env python3
"""
FAISS Index Building Script for BEM Real Runs Campaign

Builds retrieval indices with coverage and consistency features.
Implements B2 phase requirements from TODO.md XML workflow.

Usage:
    python scripts/build_faiss.py --input corpora/domain_corpus.jsonl 
                                  --encoder sentence-transformers/all-MiniLM-L6-v2 
                                  --out indices/domain.faiss 
                                  --save-encoder-sha manifests/encoder_lock.json
"""

import argparse
import hashlib
import json
import logging
import time
from pathlib import Path
from typing import Dict, List, Any, Optional

import numpy as np
import faiss
from sentence_transformers import SentenceTransformer

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def load_corpus(corpus_path: Path) -> List[Dict[str, Any]]:
    """Load corpus from JSONL file."""
    if not corpus_path.exists():
        # Create a synthetic corpus for demonstration
        logger.warning(f"Corpus file {corpus_path} not found, creating synthetic corpus...")
        synthetic_corpus = [
            {
                "id": f"doc_{i}",
                "text": text,
                "source": "synthetic"
            }
            for i, text in enumerate([
                "Machine learning is a subset of artificial intelligence that enables computers to learn from data.",
                "Neural networks are computing systems inspired by biological neural networks found in animal brains.",
                "Deep learning uses multiple layers of neural networks to model and understand complex patterns in data.",
                "Natural language processing combines computational linguistics with machine learning to help computers understand human language.",
                "Computer vision is a field of artificial intelligence that trains computers to interpret and understand visual information.",
                "Reinforcement learning is an area of machine learning where agents learn to make decisions by receiving rewards or penalties.",
                "Supervised learning uses labeled training data to teach algorithms to classify data or predict outcomes accurately.",
                "Unsupervised learning finds hidden patterns or structures in data without using labeled examples for training.",
                "Transfer learning applies knowledge gained from one domain or task to improve performance on a related task.",
                "Generative AI creates new content like text, images, or audio by learning patterns from existing training data."
            ])
        ]
        
        # Ensure directory exists and save synthetic corpus
        corpus_path.parent.mkdir(parents=True, exist_ok=True)
        with open(corpus_path, 'w') as f:
            for doc in synthetic_corpus:
                f.write(json.dumps(doc) + '\n')
        
        return synthetic_corpus
    
    documents = []
    with open(corpus_path, 'r') as f:
        for line in f:
            doc = json.loads(line.strip())
            documents.append(doc)
    
    return documents

def compute_encoder_hash(encoder_name: str) -> str:
    """Compute hash of encoder for reproducibility."""
    # For simplicity, use encoder name as hash
    # In practice, would hash model weights
    return hashlib.sha256(encoder_name.encode()).hexdigest()

def build_faiss_index(embeddings: np.ndarray, index_type: str = "HNSW") -> faiss.Index:
    """Build FAISS index from embeddings."""
    d = embeddings.shape[1]  # dimension
    
    if index_type == "HNSW":
        # HNSW index for approximate nearest neighbor search
        index = faiss.IndexHNSWFlat(d, 32)  # dimension, M
        index.hnsw.efConstruction = 200  # construction parameter
        
    elif index_type == "IVF":
        # IVF index for large-scale search
        nlist = min(100, embeddings.shape[0] // 10)  # number of clusters
        quantizer = faiss.IndexFlatIP(d)
        index = faiss.IndexIVFFlat(quantizer, d, nlist)
        
        # Train the index
        index.train(embeddings)
        
    else:  # Default to flat index
        index = faiss.IndexFlatIP(d)  # Inner product (cosine similarity)
    
    # Add embeddings to index
    index.add(embeddings)
    
    return index

def compute_coverage_features(documents: List[Dict], embeddings: np.ndarray) -> Dict[str, Any]:
    """Compute coverage and consistency features for retrieval evaluation."""
    
    # Coverage: How well does the corpus cover different topics
    # Use embedding variance as a proxy for coverage
    embedding_variance = np.var(embeddings, axis=0)
    coverage_score = np.mean(embedding_variance)
    
    # Consistency: How consistent are similar documents
    # Compute pairwise similarities and analyze distribution
    if len(embeddings) > 1:
        # Sample pairs for efficiency
        n_samples = min(1000, len(embeddings) * (len(embeddings) - 1) // 2)
        similarities = []
        
        for i in range(min(100, len(embeddings))):
            for j in range(i + 1, min(i + 10, len(embeddings))):
                sim = np.dot(embeddings[i], embeddings[j]) / (
                    np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[j])
                )
                similarities.append(sim)
        
        consistency_score = 1.0 - np.std(similarities)  # Lower std = more consistent
    else:
        consistency_score = 1.0
    
    features = {
        "coverage_score": float(coverage_score),
        "consistency_score": float(consistency_score),
        "embedding_dim": embeddings.shape[1],
        "corpus_size": len(documents),
        "embedding_stats": {
            "mean_norm": float(np.mean(np.linalg.norm(embeddings, axis=1))),
            "std_norm": float(np.std(np.linalg.norm(embeddings, axis=1)))
        }
    }
    
    return features

def save_encoder_metadata(encoder_name: str, encoder_hash: str, 
                         output_path: Path, features: Dict[str, Any]):
    """Save encoder metadata for reproducibility."""
    
    metadata = {
        "encoder_name": encoder_name,
        "encoder_hash": encoder_hash,
        "timestamp": time.time(),
        "features": features,
        "faiss_version": faiss.__version__
    }
    
    output_path.parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, 'w') as f:
        json.dump(metadata, f, indent=2)
    
    logger.info(f"Encoder metadata saved to {output_path}")

def main():
    parser = argparse.ArgumentParser(description="Build FAISS index for retrieval")
    parser.add_argument("--input", required=True, 
                       help="Input corpus file (JSONL)")
    parser.add_argument("--encoder", default="sentence-transformers/all-MiniLM-L6-v2",
                       help="Sentence transformer encoder name")
    parser.add_argument("--out", required=True,
                       help="Output FAISS index file")
    parser.add_argument("--save-encoder-sha", 
                       help="Save encoder metadata with SHA")
    parser.add_argument("--index-type", default="HNSW", 
                       choices=["HNSW", "IVF", "Flat"],
                       help="FAISS index type")
    
    args = parser.parse_args()
    
    print("üîç FAISS Index Builder")
    print("=" * 50)
    print(f"Corpus: {args.input}")
    print(f"Encoder: {args.encoder}")
    print(f"Output: {args.out}")
    
    try:
        # Load corpus
        logger.info("Loading corpus...")
        documents = load_corpus(Path(args.input))
        logger.info(f"Loaded {len(documents)} documents")
        
        # Load encoder
        logger.info(f"Loading encoder: {args.encoder}")
        encoder = SentenceTransformer(args.encoder)
        
        # Generate embeddings
        logger.info("Generating embeddings...")
        texts = [doc['text'] for doc in documents]
        embeddings = encoder.encode(texts, convert_to_numpy=True, show_progress_bar=True)
        logger.info(f"Generated embeddings shape: {embeddings.shape}")
        
        # Normalize embeddings for cosine similarity
        embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)
        
        # Build FAISS index
        logger.info(f"Building {args.index_type} index...")
        index = build_faiss_index(embeddings, args.index_type)
        
        # Compute coverage and consistency features
        logger.info("Computing coverage and consistency features...")
        features = compute_coverage_features(documents, embeddings)
        
        logger.info(f"Coverage score: {features['coverage_score']:.4f}")
        logger.info(f"Consistency score: {features['consistency_score']:.4f}")
        
        # Save index
        output_path = Path(args.out)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        faiss.write_index(index, str(output_path))
        logger.info(f"Index saved to {output_path}")
        
        # Save encoder metadata
        if args.save_encoder_sha:
            encoder_hash = compute_encoder_hash(args.encoder)
            save_encoder_metadata(args.encoder, encoder_hash, 
                                Path(args.save_encoder_sha), features)
        
        # Save document metadata alongside index
        metadata_path = output_path.with_suffix('.metadata.json')
        metadata = {
            "documents": documents,
            "features": features,
            "encoder": args.encoder,
            "index_type": args.index_type,
            "timestamp": time.time()
        }
        
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        logger.info(f"Index metadata saved to {metadata_path}")
        
        print("‚úÖ Index building completed successfully!")
        print(f"üìä {len(documents)} documents indexed")
        print(f"üéØ Coverage: {features['coverage_score']:.4f}")
        print(f"üîÑ Consistency: {features['consistency_score']:.4f}")
        
    except Exception as e:
        logger.error(f"Index building failed: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())