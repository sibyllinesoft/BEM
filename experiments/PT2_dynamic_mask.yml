base_config: performance_variant.yaml
name: PT2_dynamic_mask
description: PT2 Dynamic Rank Mask with k-hot masking and Hadamard path for efficiency
model:
  base_model: microsoft/DialoGPT-small
  architecture: bem_pt2_dynamic_mask
pt2_config:
  total_rank: 16
  active_rank: 8
  mask_temperature: 0.1
  use_hadamard_path: true
  hadamard_dim: 16
  use_instance_adaptive: true
  adaptation_dim: 32
  mask_learning_rate: 1e-3
  mask_momentum: 0.9
  straight_through: true
  max_singular_value: 1.0
  fro_budget: 0.8
  target_sparsity: 0.5
budget:
  baseline_params: 124964096
  baseline_flops: 1000000
  baseline_memory_mb: 512.0
  tolerance: 0.05
  enforce_during_training: true
  fixed_flops: true
training:
  learning_rate: 5e-4
  batch_size: 32
  max_steps: 1000
  warmup_steps: 100
  weight_decay: 1e-4
  variant_lr_multiplier: 0.5
  sparsity_annealing: true
  mask_consistency_weight: 0.1
  gradient_clip_norm: 1.0
  spectral_penalty_weight: 0.1
  budget_penalty_weight: 1.0
  sparsity_penalty_weight: 0.1
  eval_steps: 100
  save_steps: 500
  logging_steps: 50
  early_stopping_patience: 5
evaluation:
  metrics:
  - exact_match
  - f1
  - bleu
  - rouge
  - chrf
  test_split: test
  expected_improvement:
    primary_metric: f1_score
    min_improvement_pct: 1.0
  sparsity_validation:
    target_sparsity: 0.5
    tolerance: 0.1
metadata:
  variant_type: PT2
  attachment_sites:
  - all_layers
  performance_target: "+\u22651% primary metric improvement"
  budget_constraint: "Fixed FLOPs, \xB15% params vs v1.3-stack anchor"
  promotion_criteria: Pareto frontier shift OR CI-backed Slice-B gains
  special_features:
  - k_hot_masking
  - hadamard_path
  - instance_adaptive
