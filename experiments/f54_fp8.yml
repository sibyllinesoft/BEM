base_config: performance_variant.yaml
metadata:
  experiment_id: f54_fp8_generator
  description: F5.4 - FP8 quantized U,V matrices with QAT for latency reduction
  version: 1.3.0
  created: '2024-08-23'
  fast5_variant: F5.4
model:
  base_model: llama2-7b
  architecture: bem_v11_stable
  bem_config:
    sites:
    - W_O
    - W_down
    rank_schedule:
    - 2
    - 4
    - 8
    - 8
    - 8
    - 4
    - 2
    num_experts: 2
    alpha: 16.0
    dropout: 0.1
    fp8_quantization:
      enabled: true
      format: E4M3
      per_channel: true
      calibration_steps: 100
      numerical_tolerance: 1e-3
      fallback_per_tensor: true
      fake_quant_enabled: true
      observer_momentum: 0.1
      scale_init_method: max
      gradient_scaling: 1.0
      quantize_lora_A: true
      quantize_lora_B: true
      keep_codes_fp16: true
      keep_base_unchanged: true
    routing:
      chunk_size: 128
      hysteresis_tau: 0.7
      routing_type: chunk_sticky
    attention_bias:
      enabled: true
      retrieval_dim: 768
      bias_hidden_dim: 64
    governance:
      max_singular_value: 1.0
      fro_budget: 1.0
      trust_region: true
training:
  seeds:
  - 1
  - 2
  - 3
  - 4
  - 5
  learning_rate: 1e-4
  batch_size: 16
  max_steps: 5000
  warmup_steps: 500
  optimizer: adamw
  weight_decay: 0.01
  grad_clip_norm: 1.0
  fp8_qat_training:
    fp16_phase_steps: 2000
    fp16_lr: 1e-4
    qat_phase_steps: 2000
    qat_lr: 5e-5
    enable_fake_quant_at_step: 2000
    finetune_phase_steps: 1000
    finetune_lr: 1e-5
    verify_numerics_every: 100
    numerical_tolerance: 1e-3
    auto_disable_on_instability: true
  save_steps: 1000
  eval_steps: 500
data:
  train_file: data/train.jsonl
  eval_file: data/eval.jsonl
  max_seq_length: 2048
  retrieval:
    index_file: indices/retrieval.faiss
    num_retrieved: 10
    retrieval_model: sentence-transformers/all-mpnet-base-v2
evaluation:
  metrics:
  - EM
  - F1
  - BLEU
  - chrF
  slices:
    slice_a:
      name: Retrieval-Strong
      type: retrieval_strong
      coverage_threshold: 0.8
      consistency_threshold: 0.8
    slice_b:
      name: Full
      type: full
  fp8_metrics:
  - quantization_error
  - numerical_stability
  - scale_statistics
  - fake_quant_overhead
  - compression_ratio
  - quantization_noise
  performance_metrics:
  - p50_latency_ms
  - p95_latency_ms
  - throughput_tokens_per_sec
  - vram_usage_gb
  - cache_hit_rate_pct
  - memory_bandwidth_gbps
  latency_breakdown:
  - forward_pass_ms
  - quantization_overhead_ms
  - dequantization_overhead_ms
  - memory_transfer_ms
  cache_monitoring:
    enabled: true
    track_kv_hits: true
    track_routing_flips: true
    track_gate_entropy: true
    export_routing_decisions: true
system:
  device: cuda
  mixed_precision: true
  gradient_checkpointing: false
  monitor_vram: true
  monitor_latency: true
  deterministic: true
  torch_deterministic: true
  fp8_system:
    enable_tensor_cores: true
    optimize_memory_layout: true
    prefetch_quantized_weights: true
    batch_quantization_ops: true
quality_gates:
  baseline_threshold:
    enabled: true
    metrics:
    - EM
    - F1
    - BLEU
    - chrF
    min_improvement_pct: 0.0
    max_degradation_pct: 1.0
  latency_improvement:
    enabled: true
    min_improvement_pct: 3.0
    target_improvement_pct: 7.0
    metric: p50_latency_ms
  cache_performance:
    enabled: true
    min_hit_rate_pct: 80.0
  vram_budget:
    enabled: true
    max_delta_pct: 5.0
  numerical_stability:
    enabled: true
    max_quantization_error: 1e-3
    max_numerical_drift: 0.05
  memory_efficiency:
    enabled: true
    min_memory_bandwidth_improvement: 10.0
budget_validation:
  enabled: true
  param_budget_pct: 5.0
  flop_budget_pct: 5.0
  reference_config: experiments/v11_baseline.yml
  quantization_overhead_analysis: true
  memory_compression_analysis: true
logging:
  log_level: INFO
  wandb_project: bem_v13_fast5
  wandb_tags:
  - bem_v13
  - f54
  - fp8_qat
  - fast5
  - quantization
  log_metrics:
  - loss
  - eval_metrics
  - system_telemetry
  - cache_metrics
  - routing_metrics
  - fp8_metrics
  - numerical_metrics
  export_formats:
  - jsonl
  - csv
  export_routing_data: true
  export_quantization_data: true
  export_numerical_verification: true
safety:
  verify_cache_safety: true
  forbidden_sites:
  - W_Q
  - W_K
  - W_V
  - q_proj
  - k_proj
  - v_proj
  validate_rank_schedule: true
  validate_attachment_points: true
  validate_param_budget: true
  max_param_increase_pct: 5.0
  numerical_safety:
    enabled: true
    tolerance_threshold: 1e-3
    auto_fallback_per_tensor: true
    monitor_gradient_flow: true
    detect_quantization_instability: true
analysis:
  stats_config:
    n_bootstrap: 10000
    alpha: 0.05
    fdr_method: fdr_bh
  cache_analysis:
    enabled: true
    export_detailed_metrics: true
  quantization_analysis:
    enabled: true
    analyze_scale_evolution: true
    measure_compression_vs_accuracy: true
    benchmark_latency_improvements: true
    profile_memory_bandwidth: true
    compare_fp16_vs_fp8: true
  pareto_analysis:
    enabled: true
    primary_metric: F1
    efficiency_metric: latency_improvement_pct
    latency_metric: p50_latency_ms
fp8_selftest:
  enabled: true
  run_before_training: true
  test_shapes:
  - - 512
    - 64
  - - 1024
    - 128
  tolerance: 1e-3
  export_test_results: true
  auto_skip_on_failure: true
reproducibility:
  code_version: bem_v1.3.0
  config_hash: null
  python_version: null
  torch_version: null
  cuda_version: null
  data_hash: null
  index_hash: null
  fp8_implementation_hash: null
  quantization_config_hash: null
