base_config: performance_variant.yaml
metadata:
  experiment_id: qlora_baseline
  description: QLoRA - Quantized LoRA for memory-efficient parameter adaptation
  version: 1.0.0
  created: '2025-08-27'
  category: moe_lora_competitor
  reference: "QLoRA: Efficient Finetuning of Quantized LLMs (NeurIPS 2023)"
  
model:
  base_model: microsoft/DialoGPT-small
  tokenizer: microsoft/DialoGPT-small
  context_length: 4096
  mixed_precision: true
  torch_dtype: bfloat16
  architecture: qlora
  
  qlora_config:
    # Quantization parameters
    quantization:
      quantization_type: nf4  # NormalFloat 4-bit
      double_quantization: true
      compute_dtype: bfloat16
      quant_storage_dtype: uint8
      
    # LoRA parameters optimized for quantized base
    lora_rank: 16  # Higher rank to compensate for quantization
    lora_alpha: 32
    lora_dropout: 0.05
    
    # Target modules
    sites:
    - c_attn
    - c_mlp
    
    # Memory optimization
    memory_optimization:
      gradient_checkpointing: true
      paged_attention: true
      cpu_offload: false
      
    # Quantization-aware training
    qat_settings:
      quantization_aware_training: true
      fake_quantization: false
      calibration_samples: 128

training:
  seeds: [1, 2, 3, 4, 5]
  learning_rate: 2e-4  # Higher LR for quantized training
  batch_size: 4
  max_steps: 500
  warmup_steps: 50
  optimizer: paged_adamw_32bit  # QLoRA specific optimizer
  weight_decay: 0.01
  grad_clip_norm: 1.0
  save_steps: 100
  eval_steps: 100
  
  # QLoRA specific training settings
  quantized_training:
    use_nested_quant: true
    bnb_4bit_compute_dtype: bfloat16
    bnb_4bit_use_double_quant: true

data:
  train:
    jsonl_path: data/train.jsonl
  eval:
    jsonl_path: data/val.jsonl
  max_seq_length: 4096

evaluation:
  metrics:
  - EM
  - F1
  - BLEU
  - chrF
  - accuracy
  - degradation_percentage
  
  slices:
    slice_a:
      name: Retrieval-Strong
      type: retrieval_strong
      coverage_threshold: 0.8
      consistency_threshold: 0.8
    slice_b:
      name: Full
      type: full
      
  performance_metrics:
  - p50_latency_ms
  - p95_latency_ms
  - throughput_tokens_per_sec
  - vram_usage_gb
  - memory_efficiency
  - quantization_overhead
  
  robustness_metrics:
  - severe_failure_rate
  - stability_score
  - worst_case_degradation
  - quantization_noise_sensitivity

system:
  device: cuda
  mixed_precision: true
  gradient_checkpointing: true  # Essential for QLoRA
  monitor_vram: true
  monitor_latency: true
  deterministic: true
  torch_deterministic: true

logging:
  log_level: INFO
  wandb_project: bem_moe_lora_comparison
  wandb_tags:
  - qlora
  - quantized_lora
  - memory_efficient
  - production_focused
  
  log_metrics:
  - loss
  - eval_metrics
  - system_telemetry
  - memory_usage
  - quantization_metrics
  
safety:
  verify_quantization_integrity: true
  validate_memory_bounds: true
  validate_param_budget: true
  max_param_increase_pct: 3.0  # Very efficient