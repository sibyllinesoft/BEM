base_config: performance_variant.yaml
metadata:
  experiment_id: moelora_baseline
  description: MoELoRA - Mixture of Expert LoRA with traditional MoE architecture
  version: 1.0.0
  created: '2025-08-27'
  category: moe_lora_competitor
  reference: "MoELoRA: An Efficient Mixture of Experts for LoRA-based Fine-tuning (arXiv 2024)"
  
model:
  base_model: microsoft/DialoGPT-small
  tokenizer: microsoft/DialoGPT-small
  context_length: 4096
  mixed_precision: true
  torch_dtype: float16
  architecture: moelora
  
  moelora_config:
    # Traditional MoE parameters
    num_experts: 8
    expert_rank: 8
    top_k: 2
    
    # Expert configuration
    expert_config:
      initialization: kaiming_normal
      expert_dropout: 0.1
      expert_bias: false
      
    # Gating network
    gating_network:
      type: sparse_gating
      hidden_size: 256
      activation: relu
      temperature: 1.0
      
    # Load balancing
    load_balancing:
      enabled: true
      auxiliary_loss_weight: 0.01
      importance_loss_weight: 0.01
      
    # LoRA base parameters
    sites:
    - c_attn
    - c_mlp
    alpha: 16.0
    dropout: 0.1
    
    # Traditional MoE routing
    routing:
      noise_epsilon: 1e-2
      capacity_factor: 1.25
      drop_tokens: false

training:
  seeds: [1, 2, 3, 4, 5]
  learning_rate: 5e-5
  batch_size: 4
  max_steps: 500
  warmup_steps: 50
  optimizer: adamw
  weight_decay: 0.01
  grad_clip_norm: 1.0
  save_steps: 100
  eval_steps: 100
  
  # MoE specific training
  moe_training:
    auxiliary_loss_enabled: true
    expert_capacity_training: true
    load_balancing_warmup_steps: 100

data:
  train:
    jsonl_path: data/train.jsonl
  eval:
    jsonl_path: data/val.jsonl
  max_seq_length: 4096

evaluation:
  metrics:
  - EM
  - F1
  - BLEU
  - chrF
  - accuracy
  - degradation_percentage
  
  slices:
    slice_a:
      name: Retrieval-Strong
      type: retrieval_strong
      coverage_threshold: 0.8
      consistency_threshold: 0.8
    slice_b:
      name: Full
      type: full
      
  performance_metrics:
  - p50_latency_ms
  - p95_latency_ms
  - throughput_tokens_per_sec
  - vram_usage_gb
  - expert_load_balance
  - routing_efficiency
  
  robustness_metrics:
  - severe_failure_rate
  - stability_score
  - worst_case_degradation
  - expert_collapse_rate

system:
  device: cuda
  mixed_precision: true
  gradient_checkpointing: false
  monitor_vram: true
  monitor_latency: true
  deterministic: true
  torch_deterministic: true

logging:
  log_level: INFO
  wandb_project: bem_moe_lora_comparison
  wandb_tags:
  - moelora
  - mixture_of_experts
  - sparse_gating
  - moe_lora_competitor
  
  log_metrics:
  - loss
  - eval_metrics
  - system_telemetry
  - expert_utilization
  - routing_patterns
  - load_balance_metrics
  
safety:
  verify_expert_balance: true
  validate_routing_stability: true
  validate_param_budget: true
  max_param_increase_pct: 12.0