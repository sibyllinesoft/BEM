# MoLE (Mixture of LoRA Experts) Baseline Configuration
# CRITICAL: Must use identical rank and attach points as BEM for fair comparison

experiment_id: "mole_baseline"
method_type: "baseline"
approach: "mixture_of_lora_experts"

# Model Configuration
model:
  base_model: "microsoft/DialoGPT-medium"  # Must match BEM experiments
  quantization: "int8"  # Identical to BEM
  precision: "bf16"

# MoLE Configuration
mole:
  # Expert Configuration - IDENTICAL rank to BEM
  num_experts: 8  # Number of LoRA experts
  rank: 16        # r = 16 (same as BEM)
  alpha: 32       # scaling factor
  dropout: 0.1
  
  # Target modules - MUST BE IDENTICAL to BEM
  target_modules:
    - "c_attn"     # Q, K, V projections
    - "c_proj"     # Output projection (W_o)
    - "c_fc"       # MLP up projection  
    - "c_proj_mlp" # MLP down projection
  
  # Gating mechanism
  gating:
    type: "learned"  # Learned gating vs task-based
    hidden_dim: 256
    num_layers: 2
    activation: "gelu"
    dropout: 0.1
    
    # Input features for gating
    features:
      - "last_hidden_state"  # Final layer representation
      - "attention_mask"     # Sequence length info
      - "position_ids"       # Position information
    
    # Top-k expert selection
    top_k: 2  # Select top-2 experts
    temperature: 1.0
    
    # Load balancing
    load_balance_loss_weight: 0.01
    expert_capacity_factor: 1.25
  
  # Expert specialization (optional)
  expert_initialization: "random"  # vs "domain_specific"
  
# Training Configuration (identical to other baselines)
training:
  learning_rate: 2e-4
  batch_size: 8
  gradient_accumulation_steps: 4
  warmup_ratio: 0.03
  weight_decay: 0.01
  max_steps: 1000
  save_strategy: "steps"
  save_steps: 200
  evaluation_strategy: "steps"
  eval_steps: 100
  logging_steps: 50
  
  # Additional loss components
  aux_loss_weight: 0.01  # Expert utilization regularization
  
  # Optimizer
  optimizer: "adamw_torch"
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8
  max_grad_norm: 1.0

# Dataset Configuration (identical to other baselines)
datasets:
  train:
    - name: "domain_qa_train"
      path: "data/domain_qa/train.jsonl"
      split: "train"
      max_samples: 5000
    - name: "instruction_train"
      path: "data/instruction/alpaca_subset_train.jsonl"
      split: "train"
      max_samples: 3000
      
  validation:
    - name: "domain_qa_val"
      path: "data/domain_qa/val.jsonl"
      split: "validation"
    - name: "instruction_val"
      path: "data/instruction/alpaca_subset_val.jsonl"
      split: "validation"
      
  test:
    - name: "nq_open_small"
      path: "data/nq_open/test_small.jsonl"
      split: "test"
    - name: "mt_bench_subset"
      path: "data/mt_bench/subset.jsonl"
      split: "test"

# Evaluation Configuration
evaluation:
  metrics:
    - "exact_match"
    - "f1_score"
    - "bleu"
    - "chrF"
    - "json_validity"
    
  # MoLE-specific metrics
  mole_metrics:
    - "gate_entropy"           # Entropy of gating decisions
    - "expert_utilization"     # How evenly experts are used
    - "routing_accuracy"       # Correctness of expert selection
    - "load_balance_loss"      # Expert load balancing
    - "aux_loss"               # Auxiliary regularization loss
    
  # Performance metrics
  track_latency: true
  track_memory: true
  profile_tokens_per_second: true
  
# Logging Configuration
logging:
  log_dir: "logs/mole"
  wandb_project: "bem_baselines"
  wandb_name: "mole_r16_8experts"
  save_predictions: true
  save_model: true
  save_gating_decisions: true  # Log which experts are selected
  
# Expert Analysis
expert_analysis:
  track_specialization: true  # Track what each expert learns
  save_expert_outputs: true   # Save per-expert contributions
  analyze_routing_patterns: true
  
# Reproducibility
seed: 42  # Will be overridden by script with multiple seeds
deterministic: true
cuda_deterministic: true

# Hardware Configuration
hardware:
  device: "cuda"
  fp16: false
  bf16: true
  gradient_checkpointing: true
  dataloader_num_workers: 4
  
# Validation Requirements
validation:
  min_eval_samples: 100
  max_eval_samples: 1000
  eval_batch_size: 16
  
  # MoLE-specific validation
  require_expert_diversity: true
  min_expert_utilization: 0.1  # Each expert used at least 10% of time
  max_unused_experts: 2        # At most 2 experts can be rarely used